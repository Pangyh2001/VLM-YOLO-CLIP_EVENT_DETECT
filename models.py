import torch
import numpy as np
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from ultralytics import YOLOWorld
try:
    import clip
    USE_OPENAI_CLIP = True
except ImportError:
    import open_clip
    USE_OPENAI_CLIP = False
    print("âš ï¸  OpenAI CLIP not found, using open_clip instead")

from typing import List, Tuple, Dict, Any
import os

class ModelManager:
    """ç®¡ç†æ‰€æœ‰æ¨¡å‹çš„åŠ è½½å’Œæ¨ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ Using device: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_yolo()
        self._init_clip()
        self._init_vlm()
        
    def _init_yolo(self):
        """åˆå§‹åŒ– YOLO-World æ¨¡å‹"""
        print("ğŸ“¦ Loading YOLO-World...")
        model_name = self.config['models']['yolo_model']
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs("./models", exist_ok=True)
        
        # YOLO-World ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° ./models/
        self.yolo = YOLOWorld(model_name)
        print("âœ… YOLO-World loaded")
        
    def _init_clip(self):
        """åˆå§‹åŒ– CLIP æ¨¡å‹"""
        print("ğŸ“¦ Loading CLIP...")
        model_name = self.config['models']['clip_model']
        
        if USE_OPENAI_CLIP:
            # ä½¿ç”¨ OpenAI çš„ CLIP
            # æ¨¡å‹åç§°æ ¼å¼è½¬æ¢: openai/clip-vit-base-patch32 -> ViT-B/32
            model_name_lower = model_name.lower()
            
            if 'vit-base-patch32' in model_name_lower or 'vit-b-32' in model_name_lower:
                clip_name = 'ViT-B/32'
            elif 'vit-base-patch16' in model_name_lower or 'vit-b-16' in model_name_lower:
                clip_name = 'ViT-B/16'
            elif 'vit-large-patch14-336' in model_name_lower or 'vit-l-14-336' in model_name_lower:
                clip_name = 'ViT-L/14@336px'
            elif 'vit-large-patch14' in model_name_lower or 'vit-l-14' in model_name_lower:
                clip_name = 'ViT-L/14'
            elif 'rn50x64' in model_name_lower:
                clip_name = 'RN50x64'
            elif 'rn50x16' in model_name_lower:
                clip_name = 'RN50x16'
            elif 'rn50x4' in model_name_lower:
                clip_name = 'RN50x4'
            elif 'rn101' in model_name_lower:
                clip_name = 'RN101'
            elif 'rn50' in model_name_lower:
                clip_name = 'RN50'
            else:
                clip_name = 'ViT-B/32'  # é»˜è®¤
            
            print(f"   Loading CLIP model: {clip_name}")
            self.clip_model, self.clip_preprocess = clip.load(clip_name, device=self.device)
        else:
            # ä½¿ç”¨ open_clip
            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
                'ViT-B-32', pretrained='laion2b_s34b_b79k', device=self.device
            )
            self.clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')
        
        print("âœ… CLIP loaded")
        
    def _init_vlm(self):
        """åˆå§‹åŒ– VLM (Qwen2.5-VL) æ¨¡å‹"""
        print("ğŸ“¦ Loading Qwen2.5-VL...")
        model_path = self.config['models']['vlm_path']
        
        self.vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨ bfloat16ï¼ŒQwen2.5-VL æ¨è
            device_map="auto"
        )
        self.vlm_processor = AutoProcessor.from_pretrained(model_path)
        print("âœ… Qwen2.5-VL loaded")
        
    def set_yolo_classes(self, entities: List[str]):
        """è®¾ç½® YOLO è¦æ£€æµ‹çš„ç±»åˆ«"""
        self.yolo.set_classes(entities)
        
    def detect_objects(self, frame: np.ndarray, conf: float = 0.25) -> List[Dict]:
        """
        ä½¿ç”¨ YOLO æ£€æµ‹ç‰©ä½“
        è¿”å›: [{'class': str, 'bbox': [x1,y1,x2,y2], 'conf': float, 'id': int}, ...]
        """
        results = self.yolo.track(frame, conf=conf, persist=True, verbose=False)
        
        detections = []
        if results and len(results) > 0:
            result = results[0]
            if result.boxes is not None and len(result.boxes) > 0:
                boxes = result.boxes.xyxy.cpu().numpy()
                confs = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                
                # è·å–è·Ÿè¸ª IDï¼ˆå¦‚æœæœ‰ï¼‰
                ids = result.boxes.id.cpu().numpy() if result.boxes.id is not None else None
                
                for i in range(len(boxes)):
                    det = {
                        'class': self.yolo.names[int(classes[i])],
                        'bbox': boxes[i].tolist(),
                        'conf': float(confs[i]),
                        'id': int(ids[i]) if ids is not None else -1
                    }
                    detections.append(det)
                    
        return detections
    
    def compute_clip_similarity(self, image: np.ndarray, text: str) -> float:
        """
        è®¡ç®—å›¾åƒä¸æ–‡æœ¬çš„ CLIP ç›¸ä¼¼åº¦
        """
        # è½¬æ¢ä¸º PIL Image
        pil_image = Image.fromarray(image)
        
        # é¢„å¤„ç†
        image_input = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
        
        # è®¡ç®—ç‰¹å¾
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_input)
            
            if USE_OPENAI_CLIP:
                text_input = clip.tokenize([text]).to(self.device)
                text_features = self.clip_model.encode_text(text_input)
            else:
                text_input = self.clip_tokenizer([text]).to(self.device)
                text_features = self.clip_model.encode_text(text_input)
            
            # å½’ä¸€åŒ–
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = (image_features @ text_features.T).item()
            
        return similarity
    
    def vlm_verify_event(self, images: List[np.ndarray], event_name: str, 
                         positive_desc: str) -> Tuple[bool, str]:
        """
        ä½¿ç”¨ VLM éªŒè¯äº‹ä»¶æ˜¯å¦çœŸå®å‘ç”Ÿ
        è¿”å›: (æ˜¯å¦å‘ç”Ÿ, æ¨ç†ç»“æœæ–‡æœ¬)
        """
        # å‡†å¤‡æç¤ºè¯
        prompt = f"""è¯·ä»”ç»†è§‚å¯Ÿè¿™äº›å›¾åƒï¼Œåˆ¤æ–­æ˜¯å¦æ­£åœ¨å‘ç”Ÿ"{event_name}"äº‹ä»¶ã€‚

äº‹ä»¶æè¿°: {positive_desc}

è¯·å›ç­”:
1. è¿™äº›å›¾åƒä¸­æ˜¯å¦æ­£åœ¨å‘ç”Ÿä¸Šè¿°äº‹ä»¶ï¼Ÿ(æ˜¯/å¦)
2. ä½ çš„åˆ¤æ–­ä¾æ®æ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ç›´æ¥ä»¥"æ˜¯"æˆ–"å¦"å¼€å¤´å›ç­”ã€‚"""

        try:
            # å‡†å¤‡è¾“å…¥
            pil_images = [Image.fromarray(img) for img in images]
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        *[{"type": "image", "image": img} for img in pil_images],
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # å‡†å¤‡è¾“å…¥
            text = self.vlm_processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = self.vlm_processor(
                text=[text],
                images=pil_images,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                output_ids = self.vlm_model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False
                )
            
            # è§£ç 
            generated_text = self.vlm_processor.batch_decode(
                output_ids, 
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            # æå–å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥çš„ promptï¼‰
            answer = generated_text.split("assistant\n")[-1].strip()
            
            # åˆ¤æ–­æ˜¯å¦ç¡®è®¤äº‹ä»¶
            is_event = answer.startswith("æ˜¯")
            
            return is_event, answer
            
        except Exception as e:
            print(f"âŒ VLM verification error: {e}")
            return False, f"Error: {str(e)}"