import cv2
import torch
import clip
from PIL import Image
import numpy as np
import sys
import os

# === é…ç½®éƒ¨åˆ† ===
VIDEO_PATH = "/data2/pyh/video_stream_event_detection/zhongjifangan/dataset/PxTAy6kI9c4_000370_000380.mp4"
MODEL_NAME = "ViT-B/32"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ä½¿ç”¨ config.yaml ä¸­çš„è¯¦ç»†æè¿°ï¼Œæ•ˆæœæ¯”å•çº¯çš„â€œé€—çŒ«â€ä¸¤ä¸ªå­—è¦å¥½å¾—å¤š
EVENTS = {
    "è¿›é—¨å‡ºé—¨": "ä¸€ä¸ªäººæ­£åœ¨ç©¿è¿‡é—¨å£",
    "é€—çŒ«": "ä¸€ä¸ªäººæ­£åœ¨ä¸çŒ«äº’åŠ¨ç©è€",
    "èšé¤": "äººä»¬å›´ååœ¨é¤æ¡Œæ—ä¸€èµ·ç”¨é¤",
    "è·Œå€’": "ä¸€ä¸ªäººè·Œå€’åœ¨åœ°ä¸Š"
}

def main():
    print(f"ğŸ”§ Using device: {DEVICE}")
    
    # 1. æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if not os.path.exists(VIDEO_PATH):
        print(f"âŒ Error: Video file not found at {VIDEO_PATH}")
        return

    # 2. åŠ è½½ CLIP æ¨¡å‹
    print(f"ğŸ“¦ Loading CLIP model: {MODEL_NAME}...")
    try:
        model, preprocess = clip.load(MODEL_NAME, device=DEVICE)
        print("âœ… CLIP loaded successfully")
    except Exception as e:
        print(f"âŒ Failed to load CLIP: {e}")
        print("æç¤º: è¯·ç¡®ä¿å·²å®‰è£… openai-clip: pip install git+https://github.com/openai/CLIP.git")
        return

    # 3. é¢„è®¡ç®—æ–‡æœ¬ç‰¹å¾ (Text Embeddings)
    print("ğŸ“ Encoding text descriptions...")
    text_features_dict = {}
    
    model.eval()
    with torch.no_grad():
        for name, desc in EVENTS.items():
            # Tokenize
            text_inputs = clip.tokenize([desc]).to(DEVICE)
            # Encode
            text_feat = model.encode_text(text_inputs)
            # å½’ä¸€åŒ– (å…³é”®æ­¥éª¤ï¼Œå¦åˆ™è®¡ç®—å‡ºçš„ç›¸ä¼¼åº¦æ•°å€¼ä¸å¯¹)
            text_feat /= text_feat.norm(dim=-1, keepdim=True)
            text_features_dict[name] = text_feat
    
    print(f"âœ… Encoded {len(text_features_dict)} events.")

    # 4. å¤„ç†è§†é¢‘å¸§
    cap = cv2.VideoCapture(VIDEO_PATH)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"â–¶ï¸  Processing video: {VIDEO_PATH}")
    print(f"   Total frames: {total_frames}, FPS: {fps}")
    print("-" * 60)
    print(f"{'Frame':<8} | {'Time(s)':<8} | {'è¿›é—¨å‡ºé—¨':<10} | {'é€—çŒ«':<10} | {'èšé¤':<10} | {'è·Œå€’':<10}")
    print("-" * 60)

    frame_idx = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # === æ ¸å¿ƒä¿®å¤æ­¥éª¤ï¼šBGR è½¬ RGB ===
            # OpenCV è¯»å…¥æ˜¯ BGRï¼ŒCLIP éœ€è¦ RGBã€‚å¦‚æœä¸è½¬ï¼Œåˆ†æ•°ä¼šæä½æˆ–é”™ä¹±ã€‚
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # è½¬ä¸º PIL Image å¹¶é¢„å¤„ç†
            pil_image = Image.fromarray(frame_rgb)
            image_input = preprocess(pil_image).unsqueeze(0).to(DEVICE)
            
            # è®¡ç®—å›¾åƒç‰¹å¾
            with torch.no_grad():
                image_feat = model.encode_image(image_input)
                # å½’ä¸€åŒ–
                image_feat /= image_feat.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦ (Image @ Text.T)
                # ç»“æœæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨æ¯ä¸ªäº‹ä»¶çš„åˆ†æ•°
                scores = {}
                for name, text_feat in text_features_dict.items():
                    # çŸ©é˜µä¹˜æ³•è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = (image_feat @ text_feat.T).item()
                    scores[name] = similarity
            
            # æ‰“å°ç»“æœ
            frame_time = frame_idx / fps if fps > 0 else 0
            print(f"{frame_idx:<8} | {frame_time:<8.2f} | "
                  f"{scores['è¿›é—¨å‡ºé—¨']:<10.4f} | "
                  f"{scores['é€—çŒ«']:<10.4f} | "
                  f"{scores['èšé¤']:<10.4f} | "
                  f"{scores['è·Œå€’']:<10.4f}")
            
            frame_idx += 1

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Stopped by user")
    finally:
        cap.release()
        print("-" * 60)
        print("âœ… Done.")

if __name__ == "__main__":
    main()